#!/usr/bin/env perl
# Deduplicates a list of files generated by ./quickdupe

use constant MAX_HARDLINKS => 1024;

our $total_files_removed = 0;
our $total_bytes_removed = 0;
our $groups              = 0;

our $suffix = '.' . join '', map chr(65 + rand 26), 1..16;

while (<>) {
  chomp;
  my @xs = split /\t/;
  while (@xs) {
    # Avoid linking too many files together; otherwise anyone else trying to
    # hardlink something could run into errors. (Linux seems to limit
    # #hardlinks to 65000, and this causes problems for the set of empty files
    # in /usr on my machine.)
    my ($original, @duplicates) = splice @xs, 0, MAX_HARDLINKS;
    my $size = -s $original;
    ++$groups;

    for (@duplicates) {
      # Safely relink the file by first creating a temporary link. This will
      # catch failures like cross-device hardlinks before we've unlinked the
      # source, generally avoiding data loss.
      my $temp = "$_$suffix";
      if (link $original, $temp and rename $temp, $_) {
        $total_bytes_removed += $size;
        ++$total_files_removed;
      } else {
        warn "failed to unlink/relink $original to $_ via $temp: $!";
        unlink $temp;
      }
    }

    printf STDERR "\r\033[K%dk across %d file(s) relinked within %d group(s)",
                  $total_bytes_removed / 1024,
                  $total_files_removed,
                  $groups;
  }
}

print STDERR "\n";
